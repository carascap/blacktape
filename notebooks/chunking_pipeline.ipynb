{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c1b09-7280-4c52-ad1c-f8a3c23c1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "from rich.console import Console\n",
    "\n",
    "import blacktape\n",
    "from blacktape.lib import chunks\n",
    "from blacktape.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f3e97-2edb-45dc-9731-76f2abea9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c73f23-92dd-46ef-af54-6efa794a3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "blacktape.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca468d-a895-4010-8bf0-af25f335bdba",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36df6f-ebbe-48cb-a182-696cb4b77db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = Path('../tests/data/Crime_and_Punishment.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df6a22-5269-4754-bfec-691908ad2c08",
   "metadata": {},
   "source": [
    "### Chunking Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d824ab6-34e8-4b6f-80d8-6e00e2a27103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a language model to detect sentences\n",
    "model = \"en_core_web_sm\"\n",
    "nlp = spacy.load(model, disable=[\"parser\"])\n",
    "nlp.enable_pipe(\"senter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70d723-cae4-4cb8-a59e-12867a47d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_options = {\n",
    "    'nlp': nlp,\n",
    "    'max_chunk_size': 10_000,  # Value in characters (not bytes) to stay below spaCy's max doc size of 1_000_000 characters by default\n",
    "}\n",
    "\n",
    "text_file_open_options = {\n",
    "    'encoding': \"UTF-8\",\n",
    "    'errors': \"ignore\",\n",
    "    'newline': '',  # To preserve line endings\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d4114-3714-44b2-b2ec-27724e4d29d6",
   "metadata": {},
   "source": [
    "### Target Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029941cf-806f-4bfd-b4d1-db67f5062545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity types we're interested in\n",
    "target_entities = {'PERSON', 'ORG'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0d688-b9a6-4350-a4a7-a2345393fc2c",
   "metadata": {},
   "source": [
    "### Feed chunks into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617292f1-5c5e-4732-bd59-dc28cfcdd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline model doesn't necessarily have to be the model used for chunking\n",
    "with Pipeline(spacy_model=model) as pipeline:\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    with console.status(\"[bold green]Chunking file and submitting jobs...\") as status:\n",
    "\n",
    "        processed = 0  # Cumulative string length of previous chunks\n",
    "        file_path = str(test_file.resolve())\n",
    "\n",
    "        for chunk in chunks(test_file, **chunking_options, **text_file_open_options):\n",
    "\n",
    "            # Submit NER extraction job\n",
    "            pipeline.submit_ner_job(chunk, target_entities, base_offset=processed, file=file_path)\n",
    "\n",
    "            processed += len(chunk)\n",
    "\n",
    "    # Process match results as they become available\n",
    "    for result in pipeline.results():\n",
    "\n",
    "        for match in result:\n",
    "            # Resolve offset in document\n",
    "            match[\"offset\"] = match[\"offset\"] + match.pop(\"base_offset\")\n",
    "\n",
    "            matches.append(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e2e97-a919-4bad-9afc-c32c1e387fda",
   "metadata": {},
   "source": [
    "### Validate match results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd1041-9ea0-40ba-bd66-99add57efd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire text in memory\n",
    "with test_file.open(mode=\"r\", **text_file_open_options) as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "for match in matches:\n",
    "    # View match results from the pipeline\n",
    "    for key, value in match.items():\n",
    "        print(f\"{key}:\\t\\t{value}\")\n",
    "\n",
    "    # Check what's in the original text at that offset\n",
    "    start, end = match[\"offset\"], match[\"offset\"] + len(match[\"text\"])\n",
    "    expected = full_text[start:end]\n",
    "\n",
    "    print(f\"expected:\\t{expected}\")\n",
    "\n",
    "    try:\n",
    "        assert match[\"text\"] == expected\n",
    "        print(\"âœ…\")\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08294c1-e142-40c0-8fd7-f49e1089a36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
