{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c1b09-7280-4c52-ad1c-f8a3c23c1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "from rich.console import Console\n",
    "\n",
    "import blacktape\n",
    "from blacktape.lib import chunks\n",
    "from blacktape.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f3e97-2edb-45dc-9731-76f2abea9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c73f23-92dd-46ef-af54-6efa794a3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "blacktape.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca468d-a895-4010-8bf0-af25f335bdba",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36df6f-ebbe-48cb-a182-696cb4b77db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = Path('../tests/data/Crime_and_Punishment.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df6a22-5269-4754-bfec-691908ad2c08",
   "metadata": {},
   "source": [
    "### Chunking Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d824ab6-34e8-4b6f-80d8-6e00e2a27103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a language model to detect sentences\n",
    "model = \"en_core_web_sm\"\n",
    "nlp = spacy.load(model, disable=[\"parser\"])\n",
    "nlp.enable_pipe(\"senter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70d723-cae4-4cb8-a59e-12867a47d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_read_options = {\n",
    "    'nlp': nlp,\n",
    "    'max_chunk_size': 10_000,  # Value in characters (not bytes) to stay below spaCy's max doc size of 1_000_000 characters by default\n",
    "    'encoding': \"UTF-8\",\n",
    "    'errors': \"ignore\",\n",
    "    'newline': '',  # To preserve line endings\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d4114-3714-44b2-b2ec-27724e4d29d6",
   "metadata": {},
   "source": [
    "### Target Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029941cf-806f-4bfd-b4d1-db67f5062545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity types we're interested in\n",
    "target_entities = {'PERSON', 'ORG'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0d688-b9a6-4350-a4a7-a2345393fc2c",
   "metadata": {},
   "source": [
    "### Feed chunks into a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617292f1-5c5e-4732-bd59-dc28cfcdd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline model doesn't necessarily have to be the model used for chunking\n",
    "with Pipeline(spacy_model=model) as pipeline:\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    with console.status(\"[bold green]Chunking file and submitting jobs...\") as status:\n",
    "\n",
    "        processed = 0  # Cumulative string length of previous chunks\n",
    "        file_path = str(test_file.resolve())\n",
    "\n",
    "        for chunk in chunks(test_file, **file_read_options):\n",
    "\n",
    "            # Submit NER extraction job\n",
    "            pipeline.submit_ner_job(\n",
    "                chunk, target_entities,\n",
    "                base_offset=processed,\n",
    "                file=file_path)\n",
    "\n",
    "            processed += len(chunk)\n",
    "\n",
    "    # Process match results as they become available\n",
    "    for result in pipeline.results():\n",
    "\n",
    "        for match in result:\n",
    "            # Resolve offset in document\n",
    "            match[\"offset\"] = match[\"offset\"] + match.pop(\"base_offset\")\n",
    "\n",
    "            matches.append(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e2e97-a919-4bad-9afc-c32c1e387fda",
   "metadata": {},
   "source": [
    "### Validate match results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd1041-9ea0-40ba-bd66-99add57efd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire text in memory\n",
    "with test_file.open(mode=\"r\", encoding=\"UTF-8\", errors=\"ignore\", newline=\"\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "for match in matches:\n",
    "    # View match result from the pipeline\n",
    "    pprint(match)\n",
    "\n",
    "    # Check what's in the original text at that offset\n",
    "    start, end = match[\"offset\"], match[\"offset\"] + len(match[\"text\"])\n",
    "    text_extract = full_text[start:end]\n",
    "\n",
    "    print(text_extract, end=\"\")\n",
    "\n",
    "    try:\n",
    "        assert match[\"text\"] == text_extract\n",
    "        print(\" âœ…\")\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    print(\"=======\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
